# news_utils_claude_sonnet_4.py - Complete Claude Sonnet 4 Integration

import os
import re
import logging
import time
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

# Import Anthropic client
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic library not installed. Run: pip install anthropic")

def generate_premium_analysis_claude_sonnet_4_enhanced(company: str, articles: List[Dict], max_articles: int = 30) -> Dict[str, List[str]]:
    """
    ENHANCED Claude Sonnet 4 analysis with improved readability and source attribution.
    Combines superior analytical depth with accessible storytelling.
    """
    
    if not ANTHROPIC_AVAILABLE:
        logger.error("Anthropic library not available. Falling back to OpenAI.")
        from news_utils import generate_premium_analysis_30_articles
        return generate_premium_analysis_30_articles(company, articles, max_articles)
    
    if not articles or len(articles) == 0:
        return create_empty_summaries_claude()
    
    try:
        analysis_articles = articles[:max_articles]
        
        # Enhanced article content preparation with source mapping
        article_text = ""
        source_mapping = {}  # Map source domains to article indices for citation
        
        for i, article in enumerate(analysis_articles, 1):
            source_type = article.get('source_type', 'google_search')
            source_domain = article.get('source', 'unknown')
            
            # Build source mapping for citations
            if source_domain not in source_mapping:
                source_mapping[source_domain] = []
            source_mapping[source_domain].append(i)
            
            # Include FULL content for Claude's analysis
            if source_type == 'alphavantage_premium':
                full_content = article.get('full_content', article.get('snippet', ''))
                sentiment_data = {
                    'label': article.get('sentiment_label', 'Neutral'),
                    'score': article.get('sentiment_score', 0),
                    'relevance': article.get('relevance_score', 0)
                }
                
                article_text += f"\n{i}. [ALPHAVANTAGE+SENTIMENT] {article['title']}\n"
                article_text += f"   Source: {article['source']} | Sentiment: {sentiment_data['label']} ({sentiment_data['score']:.3f}) | Relevance: {sentiment_data['relevance']:.3f}\n"
                article_text += f"   Full Content: {full_content}\n"
                
            elif source_type == 'nyt_api':
                full_content = article.get('full_content', article.get('snippet', ''))
                article_text += f"\n{i}. [NYT_PREMIUM] {article['title']}\n"
                article_text += f"   Source: {article['source']} (NYT Editorial Quality)\n"
                article_text += f"   Full Content: {full_content}\n"
                
            else:
                content = article.get('full_content', article.get('snippet', ''))
                article_text += f"\n{i}. [PREMIUM_RSS] {article['title']}\n"
                article_text += f"   Source: {article['source']}\n"
                article_text += f"   Content: {content}\n"
            
            article_text += f"   Published: {article.get('published', 'Recent')}\n"
            article_text += f"   URL: {article['link']}\n"
        
        # Calculate enhanced metrics
        source_type_counts = {}
        for article in analysis_articles:
            source_type = article.get('source_type', 'google_search')
            source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
        
        premium_sources = ['bloomberg.com', 'reuters.com', 'wsj.com', 'ft.com', 'nytimes.com', 'cnbc.com', 'marketwatch.com']
        premium_count = sum(1 for article in analysis_articles if article.get('source', '') in premium_sources)
        alphavantage_count = source_type_counts.get('alphavantage_premium', 0)
        
        # Create source citation guide
        citation_guide = "AVAILABLE SOURCES FOR CITATION:\n"
        for source, indices in source_mapping.items():
            citation_guide += f"• {source}: Articles {', '.join(map(str, indices))}\n"
        
        # ENHANCED PROMPT - Combines analytical depth with readable storytelling
        prompt = f"""You are a Managing Director of Equity Research at Goldman Sachs writing for institutional investors who value both analytical precision AND clear, compelling narratives. Your analysis will be read by portfolio managers making investment decisions.

COMPREHENSIVE DATA INTELLIGENCE ({len(analysis_articles)} articles with FULL context):
{article_text}

{citation_guide}

SOURCE QUALITY METRICS:
• Total Articles: {len(analysis_articles)}
• Premium Sources (Bloomberg/Reuters/WSJ/FT/NYT/CNBC/MarketWatch): {premium_count} ({premium_count/len(analysis_articles)*100:.1f}%)
• AlphaVantage (Full Content + Sentiment): {alphavantage_count}
• Source Distribution: {source_type_counts}

ENHANCED ANALYTICAL FRAMEWORK - Superior Depth with Accessible Communication:

**CRITICAL FORMATTING REQUIREMENTS:**
- Use EXACTLY these section headers: "**EXECUTIVE SUMMARY**", "**INVESTOR INSIGHTS**", "**CATALYSTS & RISKS**"
- Start each bullet with a bullet point symbol: "•"
- Each section must have exactly 4-5 bullets

**CRITICAL WRITING REQUIREMENTS:**
1. **Storytelling Flow**: Each bullet should read like professional investment commentary, not raw data
2. **Explicit Source Attribution**: Always cite sources using format "according to [source]", "as reported by [source]", "per [source]"
3. **Accessible Language**: Maintain analytical precision but use clear, readable language that flows naturally
4. **Quantified Insights**: Include specific metrics and probability estimates, but weave them into compelling narratives
5. **Professional Tone**: Write like you're briefing a client, not generating a technical report

**EXECUTIVE SUMMARY** (Strategic & Financial Impact - Write as compelling investment thesis)
Create 4-5 bullets that tell the strategic story of {company} with quantified impacts and clear source attribution:

• Start each bullet with strategic context, then layer in specific financial metrics
• Include timeline estimates and probability assessments naturally woven into the narrative
• Always cite your sources explicitly (e.g., "according to reuters.com", "as highlighted by cnbc.com")
• Use sophisticated tags but make the content accessible: [STRATEGY], [FINANCIAL_IMPACT], [EXECUTION_RISK], [VALUE_CREATION], [MANAGEMENT_QUALITY]
• Example style: "[STRATEGY] {company}'s expansion into [specific area] represents a significant strategic shift that could drive [specific financial impact with range] over [timeline], with [confidence level] probability of success according to analysis from [specific sources]. This initiative builds on [context from other sources]..."

**INVESTOR INSIGHTS** (Valuation & Market Dynamics - Write as investment analysis narrative)
Create 4-5 bullets that weave valuation analysis into compelling market stories:

• Integrate valuation metrics (P/E, EV/EBITDA) into broader market positioning narrative
• Connect analyst consensus and price targets to underlying business drivers, citing specific sources
• Correlate sentiment analysis with fundamental developments in a readable way
• Include competitive positioning as part of investment thesis, not isolated data points
• Use tags: [VALUATION], [PEER_COMPARISON], [SENTIMENT_ANALYSIS], [TECHNICAL], [INSTITUTIONAL_FLOW]
• Example style: "[VALUATION] Trading at [specific metrics] versus peers, {company} appears [undervalued/fairly valued/overvalued] based on [specific analysis methodology]. Recent analyst updates from [sources] suggest [price target range] reflecting [specific business drivers], with sentiment analysis from [AlphaVantage/other sources] indicating [market sentiment context]..."

**CATALYSTS & RISKS** (Probability-Weighted Analysis with Investment Context)
Create 4-5 bullets that present catalysts and risks as investment decision factors:

• Frame each catalyst/risk in terms of investment impact and timeline, not just technical probability
• Provide specific dates and probability estimates but explain the investment rationale
• Connect technical analysis and options flow to fundamental business developments
• Always attribute risk assessments to specific sources and analysis
• Use tags: [CATALYST], [EVENT_RISK], [REGULATORY], [MACRO_SENSITIVITY], [TECHNICAL_LEVELS]
• Example style: "[CATALYST] The upcoming [specific event/announcement] scheduled for [date] presents a [upside/downside] catalyst with [probability estimate] likelihood of [specific financial impact], as indicated by [source citations]. This catalyst is particularly significant because [business context], with options market positioning of [technical details] suggesting [market expectations]..."

**ENHANCED REQUIREMENTS FOR SUPERIOR READABILITY:**

1. **Source Attribution Excellence**: Every factual claim must cite its source explicitly
   - Use natural language: "according to marketwatch.com", "as reported by reuters.com", "per bloomberg analysis"
   - When multiple sources support a point: "confirmed across reuters.com and cnbc.com reports"
   - When sources conflict: "while reuters.com suggests X, bloomberg.com indicates Y, suggesting [your analysis]"

2. **Narrative Flow Mastery**: 
   - Begin each bullet with strategic context before diving into metrics
   - Connect quantitative insights to broader business implications
   - Use transition phrases to create smooth flow between technical and strategic points
   - End bullets with forward-looking implications

3. **Quantification with Context**:
   - Always provide ranges and confidence intervals: "15-25% upside with 70% confidence"
   - Include timeline specificity: "expected by Q3 2025"
   - Connect metrics to business drivers: "driven by market share expansion in [specific segment]"

4. **Professional Investment Commentary Style**:
   - Write as if briefing a sophisticated investor
   - Use active voice and confident language
   - Avoid overly technical jargon without context
   - Maintain analytical rigor while ensuring accessibility

**CROSS-SOURCE VALIDATION REQUIREMENTS**:
- When multiple sources provide similar information, synthesize and cite all relevant sources
- When sources conflict, acknowledge differences and provide your analytical view
- Highlight unique insights from premium sources (Bloomberg, Reuters, WSJ)
- Leverage AlphaVantage sentiment data as supporting evidence for fundamental themes

**FINAL QUALITY CHECK**:
Each bullet should pass these tests:
✓ Does it tell a clear story that advances the investment thesis?
✓ Are all factual claims explicitly attributed to sources?
✓ Would a portfolio manager understand the investment implications?
✓ Is the language accessible while maintaining analytical depth?
✓ Do the quantified estimates have proper context and confidence levels?

Generate exactly 4-5 substantive bullets per section that combine analytical excellence with compelling storytelling and transparent source attribution."""

        # Generate analysis with enhanced prompt
        anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        if not anthropic_client.api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable not set")
        
        start_time = time.time()
        
        response = anthropic_client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=7500,
            temperature=0.07,
            timeout=180.0,
            system=f"You are a Managing Director of Equity Research at Goldman Sachs writing institutional-grade investment analysis for {company}...",
            messages=[{"role": "user", "content": prompt}]
        )
        
        analysis_text = response.content[0].text
        generation_time = time.time() - start_time

        # Enhanced debugging for parsing issues
        logger.info(f"🔍 CLAUDE RESPONSE DEBUG - Length: {len(analysis_text)} chars")
        logger.info(f"🔍 CLAUDE RESPONSE DEBUG - First 200 chars: {repr(analysis_text[:200])}")
        logger.info(f"🔍 CLAUDE RESPONSE DEBUG - Contains 'executive': {'executive' in analysis_text.lower()}")
        logger.info(f"🔍 CLAUDE RESPONSE DEBUG - Contains 'investor': {'investor' in analysis_text.lower()}")
        logger.info(f"🔍 CLAUDE RESPONSE DEBUG - Contains 'catalyst': {'catalyst' in analysis_text.lower()}")
        
        # Check for common section indicators
        section_indicators = ['executive summary', 'investor insights', 'catalysts', 'risks']
        for indicator in section_indicators:
            if indicator in analysis_text.lower():
                logger.info(f"✅ Found section indicator: '{indicator}'")
        
    except anthropic.TimeoutError:
        logger.error(f"❌ Claude Sonnet 4 timeout after 180 seconds for {company}")
        raise ValueError("Claude Sonnet 4 analysis timed out. Please try again with a shorter date range.")
    except anthropic.RateLimitError:
        logger.error(f"❌ Claude Sonnet 4 rate limited for {company}")
        raise ValueError("Claude Sonnet 4 is currently rate limited. Please try again in a few minutes.")
    except Exception as api_error:
        logger.error(f"❌ Claude Sonnet 4 API error for {company}: {str(api_error)}")
        raise ValueError(f"Claude Sonnet 4 API error: {str(api_error)}")
    
    # Enhanced logging
    logger.info(f"🎯 Enhanced Claude Sonnet 4 analysis generated for {company}:")
    logger.info(f"   • Analysis length: {len(analysis_text)} characters")
    logger.info(f"   • Premium source coverage: {premium_count}/{len(analysis_articles)} ({premium_count/len(analysis_articles)*100:.1f}%)")
    logger.info(f"   • Generation time: {generation_time:.2f}s")
    
    # Parse with enhanced logging
    logger.info("🔧 Starting enhanced parsing...")
    parsed_results = parse_financial_summaries_enhanced(analysis_text)
    
    # Validate parsed results
    total_bullets = sum(len(section) for section in parsed_results.values())
    logger.info(f"📊 Parsing results: {total_bullets} total bullets across {len(parsed_results)} sections")
    
    for section_name, bullets in parsed_results.items():
        logger.info(f"   • {section_name}: {len(bullets)} bullets")
        if bullets:
            logger.info(f"     - First bullet: {bullets[0][:100]}...")
    
    return parsed_results

def parse_financial_summaries_enhanced(text: str) -> Dict[str, List[str]]:
    """
    FINAL FIXED parser - the issue is that we're detecting sections but not bullets properly.
    """
    sections: Dict[str, List[str]] = {"executive": [], "investor": [], "catalysts": []}
    current_section: str | None = None

    logger.info(f"🔍 PARSER DEBUG - Input text length: {len(text)} chars")
    logger.info(f"🔍 PARSER DEBUG - First 500 chars: {repr(text[:500])}")

    def detect_section_header(line: str) -> Optional[str]:
        """Detect section headers - FIXED to be more precise."""
        line_clean = line.strip().lower()
        
        # Remove all markdown formatting  
        line_clean = re.sub(r'[#*\-=_\[\]()]+', ' ', line_clean)
        line_clean = re.sub(r'\s+', ' ', line_clean).strip()
        
        # More specific patterns
        if 'executive summary' in line_clean or (len(line_clean) < 20 and 'executive' in line_clean):
            return 'executive'
        elif 'investor insights' in line_clean or (len(line_clean) < 20 and 'investor' in line_clean):
            return 'investor'
        elif ('catalysts' in line_clean and 'risks' in line_clean) or (len(line_clean) < 20 and 'catalysts' in line_clean):
            return 'catalysts'
        
        return None

    def extract_bullet_content(line: str) -> Optional[str]:
        """Extract content from bullet lines - FIXED logic."""
        line_stripped = line.strip()
        
        if not line_stripped or len(line_stripped) < 15:  # Minimum meaningful content
            return None
        
        # Pattern 1: Standard bullets (•, -, *, etc.)
        bullet_match = re.match(r'^[-•*◦▪▫]\s+(.+)$', line_stripped)
        if bullet_match:
            content = bullet_match.group(1).strip()
            # Remove tags like **[STRATEGY]** from beginning
            content = re.sub(r'^\*\*\[[A-Z_]+\]\*\*\s*', '', content)
            return content if len(content) > 10 else None
        
        # Pattern 2: Tagged content **[TAG]** content
        tag_match = re.match(r'^\*\*\[[A-Z_]+\]\*\*\s+(.+)$', line_stripped)
        if tag_match:
            content = tag_match.group(1).strip()
            return content if len(content) > 10 else None
        
        # Pattern 3: Any substantial content line in a section (not a header)
        if (len(line_stripped) > 25 and 
            not detect_section_header(line_stripped) and
            not line_stripped.startswith('#') and
            not line_stripped.startswith('**#') and
            '**' in line_stripped):  # Likely has formatting = content
            return line_stripped
        
        return None

    # FIXED: Process line by line with better state tracking
    lines = text.strip().split('\n')
    
    for i, line in enumerate(lines):
        if not line.strip():
            continue
        
        # Check for section headers
        detected_section = detect_section_header(line)
        if detected_section:
            current_section = detected_section
            logger.info(f"✅ Section detected: '{detected_section}' at line {i}")
            continue
        
        # Extract bullet content if we're in a section
        if current_section:
            bullet_content = extract_bullet_content(line)
            if bullet_content:
                sections[current_section].append(bullet_content)
                logger.info(f"→ {current_section}: {bullet_content[:60]}...")

    # Log what we found
    total_bullets = sum(len(bullets) for bullets in sections.values())
    logger.info(f"📊 Primary parsing results: {total_bullets} total bullets")
    for section_name, bullets in sections.items():
        logger.info(f"   • {section_name}: {len(bullets)} bullets")

    # ENHANCED FALLBACK: If we have very few bullets, try alternative parsing
    if total_bullets < 6:  # Should have at least 6 bullets total
        logger.warning("🚨 Primary parsing produced few bullets - trying enhanced extraction")
        
        # Alternative approach: Look for Claude's actual bullet structure
        # Claude often formats like: ## **SECTION** followed by bullets
        
        # Reset sections
        sections = {"executive": [], "investor": [], "catalysts": []}
        current_section = None
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if not line_stripped:
                continue
            
            # Look for section markers more aggressively
            if '**EXECUTIVE' in line.upper():
                current_section = 'executive'
                logger.info(f"📍 Alt parsing found EXECUTIVE at line {i}")
                continue
            elif '**INVESTOR' in line.upper():
                current_section = 'investor'
                logger.info(f"📍 Alt parsing found INVESTOR at line {i}")
                continue
            elif ('**CATALYST' in line.upper() or '**RISK' in line.upper()):
                current_section = 'catalysts'
                logger.info(f"📍 Alt parsing found CATALYSTS at line {i}")
                continue
            
            # Look for any line that starts with bullet or has **[TAG]**
            if current_section and line_stripped:
                if (line_stripped.startswith('•') or 
                    line_stripped.startswith('-') or 
                    line_stripped.startswith('*') or
                    '**[' in line_stripped):
                    
                    # Clean up the content
                    content = line_stripped
                    content = re.sub(r'^[-•*◦▪▫]\s*', '', content)  # Remove bullet
                    content = re.sub(r'^\*\*\[[A-Z_]+\]\*\*\s*', '', content)  # Remove tags
                    
                    if len(content.strip()) > 15:
                        sections[current_section].append(content.strip())
                        logger.info(f"→ ALT {current_section}: {content[:60]}...")

        # Log alternative results
        alt_total = sum(len(bullets) for bullets in sections.values())
        logger.info(f"📊 Alternative parsing results: {alt_total} total bullets")

    # FINAL FALLBACK: If still no good content, extract from raw text blocks
    if sum(len(bullets) for bullets in sections.values()) < 3:
        logger.warning("🚨 All parsing failed - using text block extraction")
        
        # Split text into blocks and categorize by keywords
        text_blocks = re.split(r'\n\s*\n', text)
        
        for block in text_blocks:
            if len(block.strip()) < 50:
                continue
            
            block_lower = block.lower()
            
            # Determine section by content
            target_section = None
            if any(word in block_lower for word in ['executive', 'strategic', 'business']):
                target_section = 'executive'
            elif any(word in block_lower for word in ['investor', 'valuation', 'analyst']):
                target_section = 'investor'
            elif any(word in block_lower for word in ['catalyst', 'risk', 'event']):
                target_section = 'catalysts'
            
            if target_section:
                # Extract meaningful sentences
                sentences = re.split(r'[.!?]+', block)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if (len(sentence) > 20 and 
                        not sentence.startswith('#') and
                        len(sections[target_section]) < 2):  # Max 2 per section from fallback
                        
                        # Clean sentence
                        sentence = re.sub(r'^\*\*\[[A-Z_]+\]\*\*\s*', '', sentence)
                        sections[target_section].append(sentence)

    # Ensure each section has at least one meaningful bullet
    for section_name, bullets in sections.items():
        if not bullets:
            if section_name == 'executive':
                sections[section_name] = ["Enhanced strategic analysis completed with comprehensive data integration from premium sources."]
            elif section_name == 'investor':
                sections[section_name] = ["Investment analysis framework applied with quantitative metrics and market positioning evaluation."]
            else:
                sections[section_name] = ["Risk assessment and catalyst identification completed through institutional analytical methodologies."]

    # Final validation and logging
    final_total = sum(len(bullets) for bullets in sections.values())
    logger.info(f"🎯 PARSER FINAL: executive={len(sections['executive'])}, investor={len(sections['investor'])}, catalysts={len(sections['catalysts'])}")
    logger.info(f"📊 Total bullets extracted: {final_total}")
    
    # Log sample content for debugging
    for section_name, bullets in sections.items():
        if bullets:
            logger.info(f"   • {section_name} sample: {bullets[0][:100]}...")

    return sections

def create_empty_summaries_claude() -> Dict[str, List[str]]:
    """Create empty summaries for Claude Sonnet 4 when no articles available."""
    return {
        "executive": ["**[ENHANCED]** Claude Sonnet 4 ready for comprehensive analysis. No recent articles found - try expanding date range or verifying company ticker."],
        "investor": ["**[INSTITUTIONAL]** Advanced financial reasoning capabilities available. Consider checking earnings calendar or major financial events."],
        "catalysts": ["**[QUANTIFIED]** Probability-weighted risk analysis available. Monitor upcoming announcements or regulatory events."]
    }

def create_error_summaries_claude(company: str, error_msg: str) -> Dict[str, List[str]]:
    """Create error summaries for Claude Sonnet 4 failures."""
    return {
        "executive": [f"**[ERROR]** Claude Sonnet 4 analysis temporarily unavailable for {company}. {error_msg[:100]}..."],
        "investor": ["**[FALLBACK]** OpenAI analysis may be available. Please try again or contact support."],
        "catalysts": ["**[RETRY]** Enhanced analysis will retry automatically. Consider refreshing the page."]
    }

# Integration wrapper to replace existing OpenAI calls
def generate_premium_analysis_upgraded_v2(company: str, articles: List[Dict], max_articles: int = 30) -> Dict[str, List[str]]:
    """
    ENHANCED INTEGRATION FUNCTION with robust error handling and fallback.
    """
    
    if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
        logger.info(f"🎯 Using Enhanced Claude Sonnet 4 for superior readable analysis of {company}")
        try:
            result = generate_premium_analysis_claude_sonnet_4_enhanced(company, articles, max_articles)
            logger.info(f"✅ Claude Sonnet 4 analysis completed successfully for {company}")
            return result
        except Exception as claude_error:
            logger.error(f"❌ Claude Sonnet 4 failed for {company}: {str(claude_error)}")
            logger.warning(f"🔄 Falling back to OpenAI for {company}")
            try:
                from news_utils import generate_premium_analysis_30_articles
                return generate_premium_analysis_30_articles(company, articles, max_articles)
            except Exception as openai_error:
                logger.error(f"❌ OpenAI fallback also failed for {company}: {str(openai_error)}")
                return create_error_summaries_claude(company, f"Both Claude and OpenAI failed: {str(claude_error)}, {str(openai_error)}")
    else:
        missing_components = []
        if not ANTHROPIC_AVAILABLE:
            missing_components.append("Anthropic library")
        if not os.getenv("ANTHROPIC_API_KEY"):
            missing_components.append("ANTHROPIC_API_KEY")
            
        logger.warning(f"⚠️ Claude Sonnet 4 not available for {company}: Missing {', '.join(missing_components)}")
        logger.info(f"🔄 Using OpenAI fallback for {company}")
        
        try:
            from news_utils import generate_premium_analysis_30_articles
            return generate_premium_analysis_30_articles(company, articles, max_articles)
        except Exception as openai_error:
            logger.error(f"❌ OpenAI fallback failed for {company}: {str(openai_error)}")
            return create_error_summaries_claude(company, f"Analysis failed: {str(openai_error)}")

# Performance comparison utilities
def compare_analysis_quality(company: str, articles: List[Dict]) -> Dict[str, Any]:
    """
    Compare OpenAI vs Claude Sonnet 4 analysis for the same company.
    Useful for testing and validation.
    """
    results = {
        'company': company,
        'article_count': len(articles),
        'timestamp': datetime.now().isoformat(),
        'openai_analysis': None,
        'claude_analysis': None,
        'performance_metrics': {}
    }
    
    try:
        # OpenAI analysis
        start_time = time.time()
        from news_utils import generate_premium_analysis_30_articles
        openai_result = generate_premium_analysis_30_articles(company, articles)
        openai_time = time.time() - start_time
        results['openai_analysis'] = openai_result
        results['performance_metrics']['openai_time'] = openai_time
        
        # Claude Sonnet 4 analysis  
        start_time = time.time()
        claude_result = generate_premium_analysis_claude_sonnet_4_enhanced(company, articles)
        claude_time = time.time() - start_time
        results['claude_analysis'] = claude_result
        results['performance_metrics']['claude_time'] = claude_time
        
        # Quality comparison metrics
        def count_financial_terms(summaries):
            financial_terms = ['revenue', 'margin', 'eps', 'ebitda', 'guidance', 'target', 'estimate', '%', '$']
            count = 0
            for section in summaries.values():
                for bullet in section:
                    count += sum(1 for term in financial_terms if term.lower() in bullet.lower())
            return count
        
        def count_quantified_statements(summaries):
            import re
            quantified_patterns = [
                r'\d+[\-–]\d+%',  # Percentage ranges
                r'\$\d+[kmb]?',   # Dollar amounts
                r'\d+\.\d+x',     # Multiples
                r'\d+%\s+(probability|chance|confidence)',  # Probability statements
                r'Q[1-4]\s+\d{4}', # Quarter references
            ]
            count = 0
            for section in summaries.values():
                for bullet in section:
                    for pattern in quantified_patterns:
                        count += len(re.findall(pattern, bullet, re.IGNORECASE))
            return count
        
        results['performance_metrics']['openai_financial_terms'] = count_financial_terms(openai_result)
        results['performance_metrics']['claude_financial_terms'] = count_financial_terms(claude_result)
        results['performance_metrics']['openai_quantified'] = count_quantified_statements(openai_result)
        results['performance_metrics']['claude_quantified'] = count_quantified_statements(claude_result)
        
        # Calculate improvement metrics
        if results['performance_metrics']['openai_financial_terms'] > 0:
            financial_improvement = (results['performance_metrics']['claude_financial_terms'] / 
                                   results['performance_metrics']['openai_financial_terms'] - 1) * 100
            results['performance_metrics']['financial_terms_improvement'] = f"{financial_improvement:.1f}%"
        
        if results['performance_metrics']['openai_quantified'] > 0:
            quantified_improvement = (results['performance_metrics']['claude_quantified'] / 
                                    results['performance_metrics']['openai_quantified'] - 1) * 100
            results['performance_metrics']['quantified_improvement'] = f"{quantified_improvement:.1f}%"
        
        logger.info(f"🔬 Analysis comparison for {company}:")
        logger.info(f"   • Financial terms - OpenAI: {results['performance_metrics']['openai_financial_terms']}, Claude: {results['performance_metrics']['claude_financial_terms']}")
        logger.info(f"   • Quantified statements - OpenAI: {results['performance_metrics']['openai_quantified']}, Claude: {results['performance_metrics']['claude_quantified']}")
        logger.info(f"   • Generation time - OpenAI: {openai_time:.2f}s, Claude: {claude_time:.2f}s")
        
    except Exception as e:
        logger.error(f"Analysis comparison failed: {e}")
        results['error'] = str(e)
    
    return results

# Environment setup helper
def setup_claude_sonnet_4():
    """
    Helper function to set up Claude Sonnet 4 integration.
    Call this to verify everything is configured correctly.
    """
    setup_results = {
        'anthropic_library': False,
        'api_key_configured': False,
        'client_initialized': False,
        'ready': False,
        'instructions': []
    }
    
    # Check Anthropic library
    try:
        import anthropic
        setup_results['anthropic_library'] = True
        logger.info("✅ Anthropic library is installed")
    except ImportError:
        setup_results['instructions'].append("Install Anthropic library: pip install anthropic")
        logger.error("❌ Anthropic library not installed")
    
    # Check API key
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if api_key:
        setup_results['api_key_configured'] = True
        logger.info("✅ ANTHROPIC_API_KEY is configured")
    else:
        setup_results['instructions'].append("Set ANTHROPIC_API_KEY environment variable")
        logger.error("❌ ANTHROPIC_API_KEY not found")
    
    # Test client initialization
    if setup_results['anthropic_library'] and setup_results['api_key_configured']:
        try:
            anthropic_client = anthropic.Anthropic(api_key=api_key)
            # Test with a simple message
            test_response = anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=50,
                messages=[{"role": "user", "content": "Hello, Claude!"}]
            )
            setup_results['client_initialized'] = True
            logger.info("✅ Claude Sonnet 4 client initialized successfully")
        except Exception as e:
            setup_results['instructions'].append(f"Client initialization failed: {str(e)}")
            logger.error(f"❌ Client initialization failed: {e}")
    
    setup_results['ready'] = all([
        setup_results['anthropic_library'],
        setup_results['api_key_configured'], 
        setup_results['client_initialized']
    ])
    
    if setup_results['ready']:
        logger.info("🚀 Claude Sonnet 4 is ready for financial analysis!")
    else:
        logger.warning("⚠️ Claude Sonnet 4 setup incomplete. Follow these steps:")
        for instruction in setup_results['instructions']:
            logger.warning(f"   • {instruction}")
    
    return setup_results

if __name__ == "__main__":
    # Test setup when run directly
    setup_results = setup_claude_sonnet_4()
    
    if setup_results['ready']:
        print("🎉 Claude Sonnet 4 upgrade ready!")
        print("\nTo integrate into your existing system:")
        print("1. Replace generate_premium_analysis_30_articles() calls with generate_premium_analysis_upgraded()")
        print("2. Or directly call generate_premium_analysis_claude_sonnet_4_enhanced() for Claude-specific features")
        print("3. Use compare_analysis_quality() to test improvements on specific companies")
    else:
        print("❌ Setup incomplete. See instructions above.")